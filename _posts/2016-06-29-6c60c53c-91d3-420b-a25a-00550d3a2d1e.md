---
layout: post
title:  搜索引擎乱七八糟
date:   2016-06-29
categories: Python
tags:  Python
---

* content
{:toc}

[IPython Notebook简介1](http://www.cnblogs.com/cbscan/p/3545084.html)


cursor = db.cursor() 其实就是用来获得python执行Mysql命令的方法,也就是





# 搜索引擎乱七八糟

标签（空格分隔）： Python

---
[python中文标准库](http://python.usyiyi.cn/python_278/library/collections.html)
[参考：python实践3：cursor() — 数据库连接操作](http://blog.sina.com.cn/s/blog_7e662b4a01012qgm.html)
[python操作MySQL数据库](http://www.cnblogs.com/rollenholt/archive/2012/05/29/2524327.html)
[搜索引擎的参考：python搜索引擎](http://www.cnblogs.com/favourmeng/archive/2012/09/20/2695514.html)
[搜索引擎（简陋版）python](http://gaolizhong666.blog.163.com/blog/static/11561504220136242819683/)
[IPython Notebook简介1](http://www.cnblogs.com/cbscan/p/3545084.html)


cursor = db.cursor() 其实就是用来获得python执行Mysql命令的方法,也就是
我们所说的操作游标
下面cursor.execute则是真正执行MySQL语句，即查询TABLE_PARAMS表的数据。
至于fetchall()则是接收全部的返回结果行 row就是在python中定义的一个变量，用来接收返回结果行的每行数据



编码工作使用urllib的函数而非urllib2


## 正则表达式
[pyhton中正则表达式参考](http://wiki.ubuntu.org.cn/Python%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%93%8D%E4%BD%9C%E6%8C%87%E5%8D%97)
.*?就意味着匹配任意数量的重复，但是在能使整个匹配成功的前提下使用最少的重复
.*在使整个表达式能得到匹配的前提下）匹配尽可能多的字符


````
import re
s="""<div class="content" title="2013-05-15 13:05:55">
给一个白富美当伴娘。。。不要割。。。据说婚车司机曾追过新娘
</div>

<div class="content" title="2017-05-55 13:25:38">

我家喵咪早上打猎回来，满身都是冰凉冰凉的霜，硬往我被窝钻，那铁头功简直神了，总是能钻进被窝，等中午大太阳了，我想摸它一把，人家高傲的很<br/>那走位，压根碰不到……


</div>

<div class="content">

小时候感觉舅舅姨夫都好博学，晚饭的时候滔滔不绝，博古通今，上知天文地理，下知鸡毛蒜皮，满嘴的国家大事……长大后才知道原来这都是喝醉了在吹牛逼呢……
<!--1456162018-->

</div>
"""

myItems = re.findall('<div class="content" title="(.*?)">(.*?)</div>',s,re.S)
# 匹配后是元祖组成的列表：[('',''),('',''),('',''),('',''),...,('','')]
print myItems

otherItems = re.findall('<div class="content">(.*?)<!--',s,re.S)
# 匹配后是列表：['','','','',...,'']
print otherItems
```


## python中字符串前缀'u'与'r'的区别：
>ython里面的字符，如果开头处有个r，比如：
(r’^time/plus/\d{1,2}/$’, hours_ahead)
说明字符串r"XXX"中的XXX是普通字符。
有普通字符相比，其他相对特殊的字符，其中可能包含转义字符，即那些，反斜杠加上对应字母，表示对应的特殊含义的，比如最常见的”\n"表示换行，"\t"表示Tab等。
而如果是以r开头，那么说明后面的字符，都是普通的字符了，即如果是“\n”那么表示一个反斜杠字符，一个字母n，而不是表示换行了。
以r开头的字符，常用于正则表达式，对应着re模块。





>以u或U开头的字符串表示unicode字符串
Unicode是书写国际文本的标准方法。如果你想要用非英语写文本,那么你需要有一个支持Unicode的编辑器。
类似地,Python允许你处理Unicode文本——你只需要在字符串前加上前缀u或U。


## html实体符号转换为原始符号——(HTML逆转义)
`replaceTab = [("&lt;","<"),("&gt;",">"),("&amp;","&"),("&quot;","\""),("&nbsp;"," ")`
[参考链接](http://www.w3school.com.cn/html/html_entities.asp)
**HTML 实体**
>在 HTML 中，某些字符是预留的。
在 HTML 中不能使用小于号（<）和大于号（>），这是因为浏览器会误认为它们是标签。
如果希望正确地显示预留字符，我们必须在 HTML 源代码中使用字符实体（character entities）。
字符实体类似这样：
```
&entity_name;

或者

&#entity_number;
```
![1.png-13.7kB][1]
不间断空格（non-breaking space）
：  HTML 中的常用字符实体是不间断空格(& nbsp;)。
浏览器总是会截短 HTML 页面中的空格。如果您在文本中写 10 个空格，在显示该页面之前，浏览器会删除它们中的 9 个。如需在页面中增加空格的数量，您需要使用 & nbsp; 字符实体。


HTML 中有用的字符实体（表中实体名&后的空格实际使用时需要去掉）
注释：实体名称对大小写敏感
|显示结果|	描述|	实体名称|	实体编号|
|--------|-----|-------|-----|
|   |	空格|	& nbsp;|	& #160;|
|<|	小于号|	& lt;|	& #60;|
|>|	大于号	|& gt;|	& #62;|
|&	|和号|	& amp;	|& #38;|
|"	|引号|	& quot;	|& #34;|
|'	|撇号 	|& apos; (IE不支持)|	& #39;|
|￠|	分	|& cent;|	& #162;|
|£|	镑	|& pound;|	& #163;|
|¥|	日圆|	& yen;|	& #165;|
|€|	欧元|	& euro;|	& #8364;|
|§|	小节|	& sect;|	& #167;|
|©|	版权|	 & copy;|	& #169;|
|®|	注册商标|	& reg;|	& #174;|
|™|	商标|	& trade;|	& #8482;|
|×|	乘号|	& times;|	& #215;|
|÷|	除号|	& divide;|	& #247;|


## python爬虫参考过的文档
[系列：[Python]网络爬虫（一）：抓取网页的含义和URL基本构成](http://blog.csdn.net/pleasecallmewhy/article/details/8922826)

## python学习
著作权归作者所有。
商业转载请联系作者获得授权，非商业转载请注明出处。
作者：Heroicai0101
链接：https://www.zhihu.com/question/34949753/answer/61296691
来源：知乎

>楼主不要想着看几本书，一下就大神附体，搞出个大新闻，naive！老老实实把标准库和文档，多过几遍，这些都是精华啊。哔哔完毕，下面正式答题，列出个人工作中常用到的模块，欢迎补充：

- 日常处理需经常用到的模块：time、os、sys、datetime、ConfigParser、subprocess、logging、re
- 进阶必学模块如下，配合起来用，代码更精简，事半功倍，谁用谁知道！itertools、functools、collections、operator、toolz、cytoolz
- 数据库相关模块：json、pymongo、sqlite追求效率与速度（写c扩展的模块，我还没达到这能力，自行谷歌）numpy、pandas
- 最后，加深对迭代器、生成器的理解；涉猎map、reduce等函数式高逼格编程主题，多问google，搜stack overflow；掌握grep、sed、awk基本命令，配合起来用，更实在。
## urllib与urllib2区别
[转载：urllib与urllib2的学习总结(python2.7.X)](http://www.th7.cn/Program/Python/201305/135739.shtml)
>Python的urllib和urllib2模块都做与请求URL相关的操作，但他们提供不同的功能。他们两个最显着的差异如下：
　　urllib2可以接受一个Request对象，并以此可以来设置一个URL的headers，但是urllib只接收一个URL。这意味着，你不能伪装你的用户代理字符串等。
　　urllib模块可以提供进行urlencode的方法，该方法用于GET查询字符串的生成，urllib2的不具有这样的功能。这就是urllib与urllib2经常在一起使用的原因。

这两点对于用过urllib与urllib2的人来说比较好理解，但是对于没用过的还是不能有好的理解，下面参考官方的文档，把自己对urllib与urllib2的学习内容总结如下。

 - A.urllib2概述
　　urllib2模块定义的函数和类用来获取URL（主要是HTTP的），他提供一些复杂的接口用于处理： 基本认证，重定向，Cookies等。
　　urllib2支持许多的“URL schemes”（由URL中的“：”之前的字符串确定 - 例如“FTP”的URL方案如“ftp://python.org/”），且他还支持其相关的网络协议（如FTP，HTTP）。我们则重点关注HTTP。
　　在简单的情况下，我们会使用urllib2模块的最常用的方法urlopen。但只要打开HTTP URL时遇到错误或异常的情况下，就需要一些HTTP传输协议的知识。我们没有必要掌握HTTP RFC2616。这是一个最全面和最权威的技术文档，且不易于阅读。在使用urllib2时会用到HTTP RFC2616相关的知识，了解即可。
 - B.常用方法和类
　1. `urllib2.urlopen(url[, data][, timeout])`
　　urlopen方法是urllib2模块最常用也最简单的方法，它打开URL网址，url参数可以是一个字符串url或者是一个Request对象。URL没什么可说的，Request对象和data在request类中说明，定义都是一样的。
　　对于可选的参数timeout，阻塞操作以秒为单位，如尝试连接（如果没有指定，将使用设置的全局默认timeout值）。实际上这仅适用于HTTP，HTTPS和FTP连接。
　　先看只包含URL的请求例子：
```
import urllib2
response = urllib2.urlopen('http://python.org/')
html = response.read()
```
　　urlopen方法也可通过建立了一个Request对象来明确指明想要获取的url。调用urlopen函数对请求的url返回一个response对象。***这个response类似于一个file对象，所以用.read()函数可以操作这个response对象***，关于urlopen函数的返回值的使用，我们下面再详细说。
```
import urllib2
req = urllib2.Request('http://python.org/')
response = urllib2.urlopen(req)
the_page = response.read()
```
 这里用到了urllib2.Request类，对于上例，我们只通过了URL实例化了Request类的对象，其实Request类还有其他的参数。
   `class urllib2.Request(url[, data][, headers][, origin_req_host][, unverifiable])`
    Request类是一个抽象的URL请求。5个参数的说明如下
　　URL——是一个字符串，其中包含一个有效的URL。
　　data——是一个字符串，指定额外的数据发送到服务器，如果没有data需要发送可以为“None”。目前使用data的HTTP请求是唯一的。当请求含有data参数时，HTTP的请求为POST，而不是GET。数据应该是缓存在一个标准的application/x-www-form-urlencoded格式中。`urllib.urlencode()`函数用映射或2元组，返回一个这种格式的字符串。通俗的说就是如果想向一个URL发送数据（通常这些数据是代表一些CGI脚本或者其他的web应用）。对于HTTP来说这动作叫Post。例如在网上填的form（表单）时，浏览器会POST表单的内容，这些数据需要被以标准的格式编码（encode），然后作为一个数据参数传送给Request对象。Encoding是在urlib模块中完成的，而不是在urlib2中完成的。下面是个例子：
```
import urllib
import urllib2
url = 'http://www.someserver.com/cgi-bin/register.cgi'
values = {'name' : 'Michael Foord','location' : 'Northampton','language' : 'Python'}
data = urllib.urlencode(values)
req = urllib2.Request(url, data)
response = urllib2.urlopen(req)
the_page = response.read()
```
　　headers——是字典类型，头字典可以作为参数在request时直接传入，也可以把每个键和值作为参数调用`add_header()`方法来添加。作为辨别浏览器身份的`User-Agent header`是经常被用来恶搞和伪装的，因为一些HTTP服务只允许某些请求来自常见的浏览器而不是脚本，或是针对不同的浏览器返回不同的版本。例如，Mozilla Firefox浏览器被识别为“Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11”。默认情况下，urlib2把自己识别为Python-urllib/x.y（这里的xy是python发行版的主要或次要的版本号，如在Python 2.6中，urllib2的默认用户代理字符串是“Python-urllib/2.6。下面的例子和上面的区别就是在请求时加了一个headers，模仿IE浏览器提交请求。
```
import urllib
import urllib2
url = 'http://www.someserver.com/cgi-bin/register.cgi'
user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'
values = {'name' : 'Michael Foord','location' : 'Northampton','language' : 'Python' }
headers = { 'User-Agent' : user_agent }
data = urllib.urlencode(values)
req = urllib2.Request(url, data, headers)
response = urllib2.urlopen(req)
the_page = response.read()
```
 标准的headers组成是(Content-Length, Content-Type and Host)，只有在Request对象调用urlopen()（上面的例子也属于这个情况）或者OpenerDirector.open()时加入。两种情况的例子如下：
    
使用headers参数构造Request对象，如上例在生成Request对象时已经初始化header，而下例是Request对象调用add_header(key, val)方法附加header（Request对象的方法下面再介绍）：
```
import urllib2
req = urllib2.Request('http://www.example.com/')
req.add_header('Referer', 'http://www.python.org/')
r = urllib2.urlopen(req)
```
 OpenerDirector为每一个Request自动加上一个`User-Agent header`，所以第二种方法如下（urllib2.build_opener会返回一个OpenerDirector对象，关于urllib2.build_opener类下面再说）：
```
import urllib2
opener = urllib2.build_opener()
opener.addheaders = [('User-agent', 'Mozilla/5.0')]
opener.open('http://www.example.com/')
```
    最后两个参数仅仅是对正确操作第三方HTTP cookies 感兴趣，很少用到：
　　`origin_req_host`——是RFC2965定义的源交互的request-host。默认的取值是`cookielib.request_host(self)`。这是由用户发起的原始请求的主机名或IP地址。例如，如果请求的是一个HTML文档中的图像，这应该是包含该图像的页面请求的request-host。
　　`unverifiable` ——代表请求是否是无法验证的，它也是由RFC2965定义的。默认值为false。一个无法验证的请求是，其用户的URL没有足够的权限来被接受。例如，如果请求的是在HTML文档中的图像，但是用户没有自动抓取图像的权限，unverifiable的值就应该是true。
　　
 - `urllib2.install_opener(opener)和urllib2.build_opener([handler, ...])`
　　install_opener和build_opener这两个方法通常都是在一起用,也有时候build_opener单独使用来得到OpenerDirector对象。　　
　　install_opener实例化会得到OpenerDirector 对象用来赋予全局变量opener。如果想用这个opener来调用urlopen，那么就必须实例化得到OpenerDirector；这样就可以简单的调用OpenerDirector.open()来代替urlopen()。
　　build_opener实例化也会得到OpenerDirector对象，其中参数handlers可以被BaseHandler或他的子类实例化。子类中可以通过以下实例化：`ProxyHandler` (如果检测代理设置用),` UnknownHandler`, `HTTPHandler`, `HTTPDefaultErrorHandler`, `HTTPRedirectHandler`, `FTPHandler`, `FileHandler`, `HTTPErrorProcessor`。　
```
import urllib2
req = urllib2.Request('http://www.python.org/')
opener=urllib2.build_opener()
urllib2.install_opener(opener)
f = opener.open(req)
```
　　如上使用 `urllib2.install_opener()`设置 urllib2 的全局 `opener`。这样后面的使用会很方便，但不能做更细粒度的控制，比如想在程序中使用两个不同的 Proxy 设置等。比较好的做法是不使用 install_opener 去更改全局的设置，而只是直接调用 opener的open 方法代替全局的 urlopen 方法。
　　说到这Opener和Handler之间的操作听起来有点晕。整理下思路就清楚了。当获取一个URL时，可以使用一个opener（一个urllib2.OpenerDirector实例对象，可以由build_opener实例化生成）。正常情况下程序一直通过urlopen使用默认的opener（也就是说当你使用urlopen方法时，是在隐式的使用默认的opener对象），但也可以创建自定义的openers（通过操作器handlers创建的opener实例）。所有的重活和麻烦都交给这些handlers来做。每一个handler知道如何以一种特定的协议（http，ftp等等）打开url，或者如何处理打开url发生的HTTP重定向，或者包含的HTTP cookie。创建openers时如果想要安装特别的handlers来实现获取url（如获取一个处理cookie的opener，或者一个不处理重定向的opener）的话，先实例一个OpenerDirector对象，然后多次调用.add_handler(some_handler_instance)来创建一个opener。或者，你可以用build_opener，这是一个很方便的创建opener对象的函数，它只有一个函数调用。build_opener默认会加入许多handlers，它提供了一个快速的方法添加更多东西和使默认的handler失效。
install_opener如上所述也能用于创建一个opener对象，但是这个对象是（全局）默认的opener。这意味着调用urlopen将会用到你刚创建的opener。也就是说上面的代码可以等同于下面这段。这段代码最终还是使用的默认opener。一般情况下我们用build_opener为的是生成自定义opener，没有必要调用install_opener，除非是为了方便。
```
import urllib2
req = urllib2.Request('http://www.python.org/')
opener=urllib2.build_opener()
urllib2.install_opener(opener)
f = urllib2.urlopen(req)
```
- C.异常处理
　　当我们调用urllib2.urlopen的时候不会总是这么顺利，就像浏览器打开url时有时也会报错，所以就需要我们有应对异常的处理。
　　说到异常，我们先来了解返回的response对象的几个常用的方法：
　　`geturl()` — 返回检索的URL资源，这个是返回的真正url，通常是用来鉴定是否重定向的，如下面代码4行url如果等于“http://www.python.org/ ”说明没有被重定向。
　　`info()` — 返回页面的原信息就像一个字段的对象， 如headers，它以mimetools.Message实例为格式(可以参考HTTP Headers说明)。
　　`getcode()` — 返回响应的HTTP状态代码，运行下面代码可以得到code=200，具体各个code代表的意思请参见文后附录。
```
import urllib2
req = urllib2.Request('http://www.python.org/')
response=urllib2.urlopen(req)
url=response.geturl()
info=response.info()
code=response.getcode()
```
　　当不能处理一个response时，urlopen抛出一个URLError（对于python APIs，内建异常如，ValueError, TypeError 等也会被抛出。）
　　`HTTPError`是HTTP URL在特别的情况下被抛出的URLError的一个子类。下面就详细说说URLError和HTTPError。
　　URLError——handlers当运行出现问题时（通常是因为没有网络连接也就是没有路由到指定的服务器，或在指定的服务器不存在），抛出这个异常.它是IOError的子类.这个抛出的异常包括一个‘reason’ 属性,他包含一个错误编码和一个错误文字描述。如下面代码，request请求的是一个无法访问的地址，捕获到异常后我们打印reason对象可以看到错误编码和文字描述。
```
import urllib2
req = urllib2.Request('http://www.python11.org/')
try:
    response=urllib2.urlopen(req)
    except urllib2.URLError,e:
        print e.reason
        print e.reason[0]
        print e.reason[1]
```
运行结果：
![8啊.jpg-14.7kB][2]
　　`HTTPError`——HTTPError是URLError的子类。每个来自服务器HTTP的response都包含“status code”. 有时status code不能处理这个request. 默认的处理程序将处理这些异常的responses。例如，urllib2发现response的URL与你请求的URL不同时也就是发生了重定向时，会自动处理。对于不能处理的请求, urlopen将抛出HTTPError异常. 典型的错误包含‘404’ (没有找到页面), ‘403’ (禁止请求),‘401’ (需要验证)等。它包含2个重要的属性reason和code。
　　当一个错误被抛出的时候，服务器返回一个HTTP错误代码和一个错误页。你可以使用返回的HTTP错误示例。这意味着它不但具有code和reason属性，而且同时具有read，geturl，和info等方法，如下代码和运行结果。
```
import urllib2req = urllib2.Request('http://www.python.org/fish.html')try:    response=urllib2.urlopen(req)except urllib2.HTTPError,e:    print e.code    print e.reason    print e.geturl()    print e.read()
```
![5778455c4e05b293aef9a02de2017e39.jpg-82.4kB][3]
如果我们想同时处理HTTPError和URLError，因为HTTPError是URLError的子类，所以应该把捕获HTTPError放在URLError前面，如不然URLError也会捕获一个HTTPError错误，代码参考如下：
```
 import urllib2 
 req = urllib2.Request('http://www.python.org/fish.html') 
 try: 
     response=urllib2.urlopen(req) 
 except urllib2.HTTPError,e: 
     print 'The server couldn/'t fulfill the request.' 
     print 'Error code: ',e.code 
     print 'Error reason: ',e.reason    
 except urllib2.URLError,e:
     print 'We failed to reach a server.'
     print 'Reason: ', e.reason
 else:
     # everything is fine
     response.read()
```
这样捕获两个异常看着不爽，而且`HTTPError`还是`URLError`的子类，我们可以把代码改进如下：
```
1 import urllib2 
2 req = urllib2.Request('http://www.python.org/fish.html') 
3 try: 
4     response=urllib2.urlopen(req) 
5 except urllib2.URLError as e: 
6     if hasattr(e, 'reason'): 
7         #HTTPError and URLError all have reason attribute. 
8         print 'We failed to reach a server.' 
9         print 'Reason: ', e.reason
10     elif hasattr(e, 'code'):
11         #Only HTTPError has code attribute.
12         print 'The server couldn/'t fulfill the request.'
13         print 'Error code: ', e.code
14 else:
15     # everything is fine
16     response.read()
```
　　关于错误编码因为处理程序默认的处理重定向（错误码范围在300内），错误码在100-299范围内的表示请求成功，所以通常会看到的错误代码都是在400-599的范围内。具体错误码的说明看附录。
　　写到这上面多次提到了重定向，也说了重定向是如何判断的，并且程序对于重定向时默认处理的。如何禁止程序自动重定向呢，我们可以自定义HTTPRedirectHandler 类，代码参考如下：
```
class SmartRedirectHandler(urllib2.HTTPRedirectHandler):
    def http_error_301(self, req, fp, code, msg, headers):
        result = urllib2.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers)
        return result
 
    def http_error_302(self, req, fp, code, msg, headers):
        result = urllib2.HTTPRedirectHandler.http_error_302(self, req, fp, code, msg, headers)
        return result
```
附录：
```
 # Table mapping response codes to messages; entries have the 
 # form {code: (shortmessage, longmessage)}. 
 responses = { 
     100: ('Continue', 'Request received, please continue'), 
     101: ('Switching Protocols', 
           'Switching to new protocol; obey Upgrade header'), 
  
     200: ('OK', 'Request fulfilled, document follows'), 
     201: ('Created', 'Document created, URL follows'),
     202: ('Accepted',
           'Request accepted, processing continues off-line'),
     203: ('Non-Authoritative Information', 'Request fulfilled from cache'),
     204: ('No Content', 'Request fulfilled, nothing follows'),
     205: ('Reset Content', 'Clear input form for further input.'),
     206: ('Partial Content', 'Partial content follows.'),
 
     300: ('Multiple Choices',
           'Object has several resources -- see URI list'),
     301: ('Moved Permanently', 'Object moved permanently -- see URI list'),
     302: ('Found', 'Object moved temporarily -- see URI list'),
     303: ('See Other', 'Object moved -- see Method and URL list'),
     304: ('Not Modified',
           'Document has not changed since given time'),
     305: ('Use Proxy',
           'You must use proxy specified in Location to access this '
           'resource.'),
     307: ('Temporary Redirect',
           'Object moved temporarily -- see URI list'),
 
     400: ('Bad Request',
           'Bad request syntax or unsupported method'),
     401: ('Unauthorized',
           'No permission -- see authorization schemes'),
     402: ('Payment Required',
           'No payment -- see charging schemes'),
     403: ('Forbidden',
           'Request forbidden -- authorization will not help'),
     404: ('Not Found', 'Nothing matches the given URI'),
     405: ('Method Not Allowed',
           'Specified method is invalid for this server.'),
     406: ('Not Acceptable', 'URI not available in preferred format.'),
     407: ('Proxy Authentication Required', 'You must authenticate with '
           'this proxy before proceeding.'),
     408: ('Request Timeout', 'Request timed out; try again later.'),
     409: ('Conflict', 'Request conflict.'),
     410: ('Gone',
           'URI no longer exists and has been permanently removed.'),
     411: ('Length Required', 'Client must specify Content-Length.'),
     412: ('Precondition Failed', 'Precondition in headers is false.'),
     413: ('Request Entity Too Large', 'Entity is too large.'),
     414: ('Request-URI Too Long', 'URI is too long.'),
     415: ('Unsupported Media Type', 'Entity body in unsupported format.'),
     416: ('Requested Range Not Satisfiable',
           'Cannot satisfy request range.'),
     417: ('Expectation Failed',
           'Expect condition could not be satisfied.'),
 
     500: ('Internal Server Error', 'Server got itself in trouble'),
     501: ('Not Implemented',
           'Server does not support this operation'),
     502: ('Bad Gateway', 'Invalid responses from another server/proxy.'),
     503: ('Service Unavailable',
           'The server cannot process the request due to a high load'),
     504: ('Gateway Timeout',
           'The gateway server did not receive a timely response'),
     505: ('HTTP Version Not Supported', 'Cannot fulfill request.'),
     }
```


  [1]: http://static.zybuluo.com/maorongrong/w3a1h451cxx5p1er4ucvpxmb/1.png
  [2]: http://static.zybuluo.com/maorongrong/a8tc95ufio25ypsdsj8gkze3/8%E5%95%8A.jpg
  [3]: http://static.zybuluo.com/maorongrong/hhuq883ffrvyc3td3h97wd88/5778455c4e05b293aef9a02de2017e39.jpg